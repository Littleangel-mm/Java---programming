import os
import re
import time
import logging
import pandas as pd
from tqdm import tqdm

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from gensim import corpora
from gensim.models.ldamodel import LdaModel

# ---- YouTube API ----
try:
    from googleapiclient.discovery import build
except Exception:
    build = None

# ---- Selenium ----
try:
    from selenium import webdriver
    from selenium.webdriver.common.by import By
    from selenium.webdriver.common.keys import Keys
    from selenium.webdriver.chrome.service import Service
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
except Exception:
    webdriver = None

# ==============================
# ğŸ§© å‚æ•°åŒºï¼ˆå°å¤©ä½¿åªæ”¹è¿™é‡Œï¼‰
# ==============================
VIDEO_URL = "https://www.youtube.com/watch?v=Dqstaunpae0"  # ğŸ‘ˆ æ”¹è¿™é‡Œï¼šä½ çš„è§†é¢‘é“¾æ¥
API_KEY = ""  # ğŸ‘ˆ å¯é€‰ï¼šå¡«å…¥ä½ çš„ YouTube Data API v3 Key
MAX_COMMENTS = 100
OUT_FILE = "youtube_comments.csv"

# ==============================
# åˆå§‹åŒ–ç¯å¢ƒ
# ==============================
logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s: %(message)s")
nltk.download("punkt", quiet=True)
nltk.download("stopwords", quiet=True)
EN_STOPWORDS = set(stopwords.words("english"))

# ==============================
# å·¥å…·å‡½æ•°
# ==============================
def extract_video_id(url: str) -> str:
    m = re.search(r"(?:v=|/watch\?v=|youtu\.be/)([A-Za-z0-9_-]{11})", url)
    if m:
        return m.group(1)
    raise ValueError("âŒ æ— æ³•æå–è§†é¢‘IDï¼Œè¯·æ£€æŸ¥é“¾æ¥")

# ==============================
# YouTube API è·å–è¯„è®º
# ==============================
def fetch_comments_api(api_key, video_id, max_comments=100):
    if not build:
        raise RuntimeError("googleapiclient æœªå®‰è£…ï¼Œè¯· pip install google-api-python-client")
    youtube = build("youtube", "v3", developerKey=api_key)
    comments, next_page = [], None
    pbar = tqdm(total=max_comments, desc="ğŸ“¡ API æŠ“å–ä¸­")
    while len(comments) < max_comments:
        req = youtube.commentThreads().list(
            part="snippet",
            videoId=video_id,
            pageToken=next_page,
            maxResults=min(100, max_comments - len(comments)),
            textFormat="plainText",
            order="relevance"
        )
        resp = req.execute()
        for it in resp.get("items", []):
            snip = it["snippet"]["topLevelComment"]["snippet"]
            comments.append({
                "author": snip.get("authorDisplayName"),
                "text": snip.get("textDisplay"),
                "publishedAt": snip.get("publishedAt"),
                "likeCount": snip.get("likeCount", 0)
            })
            pbar.update(1)
            if len(comments) >= max_comments:
                break
        next_page = resp.get("nextPageToken")
        if not next_page:
            break
    pbar.close()
    return comments

# ==============================
# Selenium å¤‡ç”¨æŠ“å–
# ==============================
def fetch_comments_selenium(video_url, max_comments=100):
    if webdriver is None:
        raise RuntimeError("selenium æœªå®‰è£…ï¼Œè¯· pip install selenium")

    options = webdriver.ChromeOptions()
    options.add_argument("--headless=new")
    options.add_argument("--disable-gpu")
    driver = webdriver.Chrome(options=options)

    driver.get(video_url)
    time.sleep(3)
    body = driver.find_element(By.TAG_NAME, "body")

    for _ in range(5):
        body.send_keys(Keys.PAGE_DOWN)
        time.sleep(1)

    comments, seen = [], set()
    pbar = tqdm(total=max_comments, desc="ğŸŒ€ Selenium æŠ“å–ä¸­")
    while len(comments) < max_comments:
        elems = driver.find_elements(By.CSS_SELECTOR, "#content-text")
        for el in elems:
            text = el.text.strip()
            if text and text not in seen:
                comments.append({"text": text})
                seen.add(text)
                pbar.update(1)
                if len(comments) >= max_comments:
                    break
        driver.execute_script("window.scrollBy(0, 1000);")
        time.sleep(1)
        if len(comments) >= max_comments:
            break
    driver.quit()
    pbar.close()
    return comments

# ==============================
# æ–‡æœ¬å¤„ç† & LDA æ¨¡å‹
# ==============================
def preprocess_texts(texts):
    cleaned = []
    for t in texts:
        s = re.sub(r"http\S+", "", t.lower())
        s = re.sub(r"[^a-z0-9\s']", " ", s)
        tokens = word_tokenize(s)
        tokens = [w for w in tokens if w not in EN_STOPWORDS and len(w) > 2]
        cleaned.append(tokens)
    return cleaned

def lda_analysis(tokenized_texts, num_topics=5):
    dictionary = corpora.Dictionary(tokenized_texts)
    dictionary.filter_extremes(no_below=2, no_above=0.8)
    corpus = [dictionary.doc2bow(text) for text in tokenized_texts]

    lda = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, passes=10, random_state=42)
    print("\nğŸ§  LDA æ¨¡å‹ä¸»é¢˜ï¼š")
    for idx, topic in lda.print_topics(num_words=8):
        print(f"ä¸»é¢˜ {idx+1}: {topic}")
    return lda, dictionary, corpus

# ==============================
# ä¸»ç¨‹åº
# ==============================
if __name__ == "__main__":
    video_id = extract_video_id(VIDEO_URL)
    print(f"ğŸ¬ æ­£åœ¨çˆ¬å–è§†é¢‘è¯„è®ºï¼š{VIDEO_URL}\n")

    if API_KEY:
        try:
            comments = fetch_comments_api(API_KEY, video_id, MAX_COMMENTS)
        except Exception as e:
            logging.warning(f"API æŠ“å–å¤±è´¥ï¼š{e}ï¼Œæ”¹ç”¨ Seleniumã€‚")
            comments = fetch_comments_selenium(VIDEO_URL, MAX_COMMENTS)
    else:
        comments = fetch_comments_selenium(VIDEO_URL, MAX_COMMENTS)

    if not comments:
        print("âŒ æ²¡æŠ“åˆ°è¯„è®ºï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–è§†é¢‘é“¾æ¥ã€‚")
        exit()

    # ä¿å­˜ CSV
    df = pd.DataFrame(comments)
    df.to_csv(OUT_FILE, index=False, encoding="utf-8-sig")
    print(f"âœ… å·²ä¿å­˜ {len(df)} æ¡è¯„è®ºåˆ°æ–‡ä»¶ï¼š{OUT_FILE}")

    # è¿›è¡Œ LDA åˆ†æ
    texts = [c["text"] for c in comments if c.get("text")]
    tokenized = preprocess_texts(texts)
    lda_analysis(tokenized, num_topics=5)
    print("\nâœ¨ å…¨éƒ¨å®Œæˆï¼")
